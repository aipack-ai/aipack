# This `~/.aipack-base/config.toml` file is the base config for all of the aipack workspaces (`.aipack/` container folders)

# Note: Was `default_options` in `<= 0.7.9` (both still supported, but now use 'options')
[options]

# `model` required to be able to run an agent. 
#         This will be the fallback for any workspace that does not define its model in their config.toml
#         (any model supported by the Rust genai crate)
#
# Support models from: OpenAI, Gemini, Anthropic, Ollama (local), DeepSeek, Groq, Grok(xAI)
#
# e.g.,  
#    Ollama: "phi4:14b", "gemma3:4b", "gemma3:27b", "deepseek-r1:70b", "llama4:scout" (or any locally installed Ollama)
# Anthropic: "claude-sonnet-4-20250514", "claude-3-5-haiku-latest"
#    OpenAI: "gpt-4.1, "gpt-4.1-mini", "gpt-4.1-nano", "o4-mini", "o4-mini-high", "o4-mini-low"
#    Gemini: "gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-zero"

model = "gpt-4.1-mini" # or an alias from below (e.g., "flash", "gpro")

# Temperature (by default unset)

# temperature = 0.0

# How many inputs can be processed at the same time (Defaults to 2 if absent)

input_concurrency = 2

# Model Aliases
#
# Define your own model aliases for any model/provider you have access to, and they can be used in place of the model name.
# This can also be overridden or complemented in the `# Options` section of the aipack, or in the `options` return of before all or data
#
# Obviously, agent, and sub config can use full model names, and even namspaced with supported providers (e.g., `fireworks::...`)
#
# Note: It is important to have `model_aliases` as a property of `options.model_aliases` as shown below.
#
# Change as you see fit. Can be overridden in workspace config.toml `.aipack/config.toml`.
[options.model_aliases]
# -- Anthropic
claude        = "claude-sonnet-4-20250514"
# -- OpenAI
gpt           = "gpt-4.1"
gpt-mini      = "gpt-4.1-mini"
gpt-nano      = "gpt-4.1-nano"
# For openai, Can also use the `-high`, `-medium`, `-low` prefix.
#             e.g. o3-low, o4-mini-high
# -- Google
gpro          = "gemini-2.5-pro"
gpro-low      = "gemini-2.5-pro-low"
gpro-medium   = "gemini-2.5-pro-medium"
gpro-high     = "gemini-2.5-pro-high"
flash         = "gemini-2.5-flash"
flash-zero    = "gemini-2.5-flash-zero"
flash-low     = "gemini-2.5-flash-low"
flash-medium  = "gemini-2.5-flash-medium"
flash-high    = "gemini-2.5-flash-high"
lite          = "gemini-2.5-flash-lite-preview-06-17"
# -- Fireworks
qwen3-coder   = "fireworks::qwen3-coder-480b-a35b-instruct"
qwen3         = "fireworks::qwen3-235b-a22b-instruct-2507"
qwen3-thinker = "fireworks::accounts/fireworks/models/qwen3-235b-a22b-thinking-2507"
kimi-k2       = "fireworks::kimi-k2-instruct"
glm           = "fireworks::glm-4p5"
glm-air       = "fireworks::glm-4p5-air"
# -- Deepseek (from deepseek.com)
r1            = "deepseek-reasoner"
