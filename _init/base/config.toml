# This `~/.aipack-base/config.toml` file is the base config for all of the aipack workspaces (`.aipack/` container folders)

[default_options]
# `model` required to be able to run an agent. 
#         This will be the fallback for any workspace that does not define its model in their config.toml
#         (any model supported by the Rust genai crate)
#
# e.g.,    
#          Ollama: "llama3.3:70b", "llama3.1:8b", "llama3.2:3b", "deepseek-r1:8b", "deepseek-coder-v2:16b" (or any locally installed Ollama)
#            Groq: "deepseek-r1-distill-llama-70b", "llama3-8b-8192", "llama-3.3-70b-versatile"
#             xAI: "grok-beta"
#        DeepSeek: "deepseek-chat", "deepseek-reasoner" (from deepseek.com)
#          Gemini: "gemini-2.0-flash", "gemini-2.0-pro-exp-02-05", "gemini-2.0-flash-lite"
#       Anthropic: "claude-3-7-sonnet-latest", "claude-3-5-haiku-latest"
#          OpenAI: "o3-mini", "o3-mini-high", "o3-mini-low", "gpt-4o", "gpt-4o-mini"
model = "gpt-4o-mini" # or an alias below (e.g. "flash", "claude")

# Temperature (by default unset)
#
# temperature = 0.0

# How many inputs can be processed at the same time (Defaults to 1 if absent)
#
input_concurrency = 2


# Model Aliases
# Define your own model aliases for any model/provider you have access to, and they can be used in place of the model name.
# This can also be overridden or complemented in the `# Options` section of the aipack.
# Note: It is important to have `model_aliases` as a property of `default_options` as shown below.
#
[default_options.model_aliases]
pro      = "claude-3-7-sonnet-latest"
claude   = "claude-3-7-sonnet-latest"
high     = "o3-mini-high"
med      = "o3-mini"
low      = "o3-mini-low"
standard = "gpt-4o"
cheap    = "gpt-4o-mini"
flash    = "gemini-2.0-flash"
fast     = "gemini-2.0-flash"
r1       = "deepseek-reasoner"
