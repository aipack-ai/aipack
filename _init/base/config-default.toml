# >>> DO NOT EDIT THIS FILE <<<

# IMPORTANT: THIS FILE WILL BE UPDATED WITH EACH AIPACK UPDATE

# To customize AIPACK, edit the sibling `./config-user.toml` with the values to add or override.

# Thus, `config-default.toml` will be loaded first, and `config-user.toml` will be merged on top of it to create the
# global configuration for aipack. This can then be overridden by the workspace `.aipack/config.toml` and agent-specific options.

[options]

# `model` is required to run an agent.
#         This will be the fallback if not defined in `./config-user.toml` or a workspace-level `config.toml`.
#
# Supported models from:
#   - OpenAI, Gemini, Anthropic, Ollama (local), xAI, Cohere, DeepSeek,
#   - Fireworks.Ai, Nebius, Groq, Together.ai
#   - For OpenAI & Gemini models, you can use `-low`, `-medium`, `-high` suffixes to control reasoning budget (e.g., `gpt-5-mini-high`)
#   - AIPACK with the Genai library will try to match the appropriate provider; if not found, it will fallback to localhost ollama
#   - Can specify a model with namespacing for example `fireworks::kimi-k2-instruct`
#
# For reasoning model, like gpt-5, gemini-2.5-flash, and claude-sonnet-4-5-20250929, reasoning suffixes, 
# such as `-minimal`, `-low`, `-medium`, `-high` suffixes (or event  or `-zero` for flash) can be used on model name or even model aliases
#
# e.g.,
#    OpenAI: "gpt-5", "gpt-5-mini", "gpt-5-nano" (with optional `-low`, `-medium`, `-high` suffixes)
#    Gemini: "gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite", "gemini-2.5-flash-zero" (with `-zero` for flash, `-low`, `-medium`, `-high` suffixes)
# Anthropic: "claude-sonnet-4-5-20250929"
# Fireworks: "fireworks::qwen3-coder-480b-a35b-instruct", "fireworks::kimi-k2-instruct", or full name `accounts/fireworks/models/qwen3-235b-a22b-thinking-2507`
#    Ollama: "phi4:14b", "gemma3:4b", "gemma3:27b", "deepseek-r1:70b", "llama4:scout" (or any locally installed Ollama)

model = "gpt-5-mini" # or an alias from below (e.g., "flash", "gpro")

# Temperature (by default unset)

# temperature = 0.0

# How many inputs can be processed at the same time (Defaults to 2 if absent)

input_concurrency = 2

# Model Aliases
#
# These are default model aliases that can be used.
#
# These will be updated over time as versions are updated.
#
# Customize global model aliases in `./config-user.toml`.
#
# Recommendation: Use simple alias names with `_` and `-` (like those below or in `config-default.toml`).
#
# Note 1: If an alias name contains `.` or special characters, use double quotes.
#         e.g., `"my-nano" = "gpt-5-nano"`
#
# Note 2: Aliases can be added or overridden in the workspace `.aipack/config.toml` or via agent options.
#
# Note 3: for reasoning model, reasoning suffix `-minimal`, `-low`, `-medium`, `-high` can be added and 
#         those will be added back to the model name. 
#         For example a `sonnet-low` as model name will be translated to 
#
[options.model_aliases]
# -- Anthropic
claude        = "claude-sonnet-4-5-20250929"
sonnet        = "claude-sonnet-4-5-20250929"
haiku         = "claude-haiku-4-5-20251001"
opus          = "claude-opus-4-1-20250805"
# -- OpenAI
# For OpenAI, model names can use `-high`, `-medium`, or `-low` suffixes.
#             e.g., `gpt-5-low`, `gpt-5-nano-high`
gpt           = "gpt-5"
mini          = "gpt-5-mini"
nano          = "gpt-5-nano"
codex         = "gpt-5-codex"
# -- Google
# As shown below, Gemini model names also support the use of `-high`, `-medium`, or `-low` suffixes,
# in addition to the `-zero` suffix for Flash 2.5
gpro          = "gemini-pro-latest"
flash         = "gemini-flash-latest"
lite          = "gemini-flash-lite-latest"
# -- Fireworks (namespaced with `fireworks::_model_name`
qwen-coder    = "fireworks::qwen3-coder-480b-a35b-instruct"
qwen          = "fireworks::qwen3-235b-a22b-instruct-2507"
qwen-thinker  = "accounts/fireworks/models/qwen3-235b-a22b-thinking-2507" # can be full fireworks name
kimi          = "fireworks::kimi-k2-instruct"
glm           = "fireworks::glm-4p5"
glm-air       = "fireworks::glm-4p5-air"
gpt-oss       = "fireworks::gpt-oss-120b"
gpt-oss-120b  = "fireworks::gpt-oss-120b"
gpt-oss-small = "fireworks::gpt-oss-20b"
gpt-oss-20b   = "fireworks::gpt-oss-20b"
# -- DeepSeek (from deepseek.com)
r1            = "deepseek-reasoner"