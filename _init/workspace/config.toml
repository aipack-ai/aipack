# This `.aipack/config.toml` file overrides the base `~/.aipack-base/config.toml`.
# Any property from the base config.toml can be overridden here for this workspace (the workspace is the parent directory of this .aipack/ directory).

[default_options]

# `model` is required (any model supported by the Rust genai crate)
#         By default, it is set in the `~/.aipack-base/config.toml`, but it can be overridden here,
#         as with any other properties.
# e.g.,
#    Ollama: "llama3.3:70b", "llama3.1:8b", "llama3.2:3b", "deepseek-r1:8b", "deepseek-coder-v2:16b" (or any locally installed Ollama)
#      Groq: "deepseek-r1-distill-llama-70b", "llama3-8b-8192", "llama-3.3-70b-versatile"
#       xAI: "grok-3-beta", "grok-3-fast-beta", "grok-3-mini-beta", "grok-3-mini-fast-beta"
#  DeepSeek: "deepseek-chat", "deepseek-reasoner" (from deepseek.com)
#    Gemini: "gemini-2.5-pro-exp-03-25", "gemini-2.0-flash", "gemini-2.0-flash-lite"
# Anthropic: "claude-3-7-sonnet-latest", "claude-3-5-haiku-latest"
#    OpenAI: "o4-mini", "o4-mini-high", "o4-mini-low", "gpt-4.1, "gpt-4.1-mini",  "gpt-4.1-nano"
#
# model = "gpt-4.1-mini" # or an alias from below (e.g., "flash", "gpro")

# Temperature - by default unset
#
# temperature = 0.0

# Concurrency - How many inputs can be processed at the same time
# (Set to 2 in the default ~/.aipack-base/config.toml if absent)
#
# input_concurrency = 6

# Model Aliases - or override model aliases
# The ones below are already configured in the ~/.aipack-base/config.toml
# However, you can change or add the mapping for this workspace
#
# [default_options.model_aliases]
# pro      = "claude-3-7-sonnet-latest"
# claude   = "claude-3-7-sonnet-latest"
# high     = "o3-mini-high"
# med      = "o3-mini"
# low      = "o3-mini-low"
# standard = "gpt-4o"
# cheap    = "gpt-4o-mini"
# flash    = "gemini-2.0-flash"
# fast     = "gemini-2.0-flash"
# r1       = "deepseek-reasoner"